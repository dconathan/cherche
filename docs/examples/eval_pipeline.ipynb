{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semanlink automatic tagging and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents how to evaluate a neural search pipeline using pairs of queries and answers. We will automatically tag arXiv papers that François-Paul Servant manually automated as part of the [Semanlink](http://www.semanlink.net/sl/home?lang=fr) Knowledge Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint as print\n",
    "from cherche import data, rank, retrieve, eval\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents, query_answers = data.arxiv_tags(arxiv_title=True, arxiv_summary=False, comment=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `documents` contain a list of tags. Each tag is represented as a dictionary and contains a set of attributes. We will try to automate the tagging of arXiv documents with a neural search pipeline that will retrieve tags based on their attributes using the title, abstract, and comments of the arXiv articles as a query. For each query, there is a list of relevant document identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' Joint Embedding of Words and Labels for Text Classification',\n",
      "  [{'uri': 'http://www.semanlink.net/tag/deep_learning_attention'},\n",
      "   {'uri': 'http://www.semanlink.net/tag/arxiv_doc'},\n",
      "   {'uri': 'http://www.semanlink.net/tag/nlp_text_classification'},\n",
      "   {'uri': 'http://www.semanlink.net/tag/label_embedding'}]),\n",
      " (' A Survey on Recent Approaches for Natural Language Processing in '\n",
      "  'Low-Resource Scenarios',\n",
      "  [{'uri': 'http://www.semanlink.net/tag/bosch'},\n",
      "   {'uri': 'http://www.semanlink.net/tag/survey'},\n",
      "   {'uri': 'http://www.semanlink.net/tag/arxiv_doc'},\n",
      "   {'uri': 'http://www.semanlink.net/tag/nlp_low_resource_scenarios'},\n",
      "   {'uri': 'http://www.semanlink.net/tag/low_resource_languages'}])]\n"
     ]
    }
   ],
   "source": [
    "print(query_answers[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the list of attributes each tag has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prefLabel': ['Attention mechanism'],\n",
       " 'type': ['http://www.semanlink.net/2001/00/semanlink-schema#Tag'],\n",
       " 'broader': ['http://www.semanlink.net/tag/deep_learning'],\n",
       " 'creationTime': '2016-01-07T00:58:24Z',\n",
       " 'creationDate': '2016-01-07',\n",
       " 'comment': 'Good explanation is this [blog post by D. Britz](/doc/?uri=http%3A%2F%2Fwww.wildml.com%2F2016%2F01%2Fattention-and-memory-in-deep-learning-and-nlp%2F). (But the best explanation related to attention is to be found in this [post](/doc/2019/08/transformers_from_scratch_%7C_pet) about Self-Attention.) \\r\\n\\r\\nWhile simple Seq2Seq builds a single context vector out of the encoder’s last hidden state, attention creates\\r\\nshortcuts between the context vector and the entire source input: the context vector has access to the entire input sequence.\\r\\nThe decoder can “attend” to different parts of the source sentence at each step of the output generation, and the model learns what to attend to based on the input sentence and what it has produced so far.\\r\\n\\r\\nPossible to interpret what the model is doing by looking at the Attention weight matrix\\r\\n\\r\\nCost: We need to calculate an attention value for each combination of input and output word (D. Britz: -> \"attention is a bit of a misnomer: we look at everything in details before deciding what to focus on\")\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n',\n",
       " 'uri': 'http://www.semanlink.net/tag/deep_learning_attention',\n",
       " 'broader_prefLabel': ['Deep Learning'],\n",
       " 'broader_related': ['http://www.semanlink.net/tag/feature_learning',\n",
       "  'http://www.semanlink.net/tag/feature_extraction'],\n",
       " 'broader_prefLabel_text': 'Deep Learning',\n",
       " 'prefLabel_text': 'Attention mechanism'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate a first piepline made of a single retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Precision@1': '63.06%',\n",
       " 'Precision@2': '43.47%',\n",
       " 'Precision@3': '33.12%',\n",
       " 'Precision@4': '26.67%',\n",
       " 'Precision@5': '22.55%',\n",
       " 'Recall@1': '16.79%',\n",
       " 'Recall@2': '22.22%',\n",
       " 'Recall@3': '25.25%',\n",
       " 'Recall@4': '27.03%',\n",
       " 'Recall@5': '28.54%',\n",
       " 'F1@1': '26.52%',\n",
       " 'F1@2': '29.41%',\n",
       " 'F1@3': '28.65%',\n",
       " 'F1@4': '26.85%',\n",
       " 'F1@5': '25.19%',\n",
       " 'R-Precision': '26.95%',\n",
       " 'Precision': '5.65%'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = retrieve.TfIdf(\n",
    "    key = \"uri\",\n",
    "    on = [\"prefLabel_text\", \"altLabel_text\"], \n",
    "    documents = documents,\n",
    "    tfidf = TfidfVectorizer(lowercase=True, min_df=1, max_df=0.9, ngram_range=(3, 7), analyzer=\"char\"), \n",
    "    k = 30,\n",
    ")\n",
    "\n",
    "eval.eval(search = retriever, query_answers=query_answers, hits_k=range(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of Lunr are inferior to TfIdf on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Precision@1': '56.55%',\n",
       " 'Precision@2': '42.10%',\n",
       " 'Precision@3': '34.17%',\n",
       " 'Precision@4': '27.80%',\n",
       " 'Precision@5': '23.33%',\n",
       " 'Recall@1': '14.90%',\n",
       " 'Recall@2': '21.52%',\n",
       " 'Recall@3': '25.86%',\n",
       " 'Recall@4': '27.74%',\n",
       " 'Recall@5': '28.59%',\n",
       " 'F1@1': '23.59%',\n",
       " 'F1@2': '28.48%',\n",
       " 'F1@3': '29.44%',\n",
       " 'F1@4': '27.77%',\n",
       " 'F1@5': '25.69%',\n",
       " 'R-Precision': '27.59%',\n",
       " 'Precision': '14.00%'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = retrieve.Lunr(\n",
    "    key = \"uri\",\n",
    "    on = [\"prefLabel_text\", \"altLabel_text\"], \n",
    "    documents = documents,\n",
    "    k = 10,\n",
    ")\n",
    "\n",
    "eval.eval(search = retriever, query_answers=query_answers, hits_k=range(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find an explanation of the metrics [here](https://amitness.com/2020/08/information-retrieval-evaluation/). The TfIdf retriever using caracters ngrams did well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what tagging looks like using our retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'uri': 'http://www.semanlink.net/tag/information_retrieval',\n",
       "  'similarity': 4.147},\n",
       " {'uri': 'http://www.semanlink.net/tag/dense_passage_retrieval',\n",
       "  'similarity': 3.489},\n",
       " {'uri': 'http://www.semanlink.net/tag/ranking_information_retrieval',\n",
       "  'similarity': 3.489},\n",
       " {'uri': 'http://www.semanlink.net/tag/embeddings_in_ir', 'similarity': 3.489},\n",
       " {'uri': 'http://www.semanlink.net/tag/retrieval_augmented_lm',\n",
       "  'similarity': 3.489},\n",
       " {'uri': 'http://www.semanlink.net/tag/retrieval_based_nlp',\n",
       "  'similarity': 3.489},\n",
       " {'uri': 'http://www.semanlink.net/tag/entity_discovery_and_linking',\n",
       "  'similarity': 1.579},\n",
       " {'uri': 'http://www.semanlink.net/tag/neural_models_for_information_retrieval',\n",
       "  'similarity': 1.479}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever(\"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to improve those results using a ranker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = retrieve.TfIdf(\n",
    "    key = \"uri\",\n",
    "    on = [\"prefLabel_text\", \"altLabel_text\"], \n",
    "    documents = documents,\n",
    "    tfidf = TfidfVectorizer(lowercase=True, min_df=1, max_df=0.9, ngram_range=(3, 7), analyzer=\"char\"), \n",
    "    k = 30,\n",
    ")\n",
    "\n",
    "ranker = rank.Encoder(\n",
    "    key = \"uri\",\n",
    "    on = [\"prefLabel_text\", \"altLabel_text\"],\n",
    "    encoder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\").encode,\n",
    "    k = 10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranker embeddings calculation.: 100%|█| 7/7 [00:02<00:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfIdf retriever\n",
       " \t key: uri\n",
       " \t on: prefLabel_text, altLabel_text\n",
       " \t documents: 433\n",
       "Encoder ranker\n",
       "\t key: uri\n",
       "\t on: prefLabel_text, altLabel_text\n",
       "\t k: 10\n",
       "\t similarity: cosine\n",
       "\t Embeddings pre-computed: 433"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search = retriever + ranker\n",
    "search.add(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Precision@1': '63.69%',\n",
       " 'Precision@2': '42.83%',\n",
       " 'Precision@3': '33.01%',\n",
       " 'Precision@4': '26.75%',\n",
       " 'Precision@5': '22.55%',\n",
       " 'Recall@1': '17.15%',\n",
       " 'Recall@2': '22.64%',\n",
       " 'Recall@3': '25.75%',\n",
       " 'Recall@4': '27.34%',\n",
       " 'Recall@5': '28.55%',\n",
       " 'F1@1': '27.02%',\n",
       " 'F1@2': '29.63%',\n",
       " 'F1@3': '28.93%',\n",
       " 'F1@4': '27.04%',\n",
       " 'F1@5': '25.20%',\n",
       " 'R-Precision': '27.43%',\n",
       " 'Precision': '13.18%'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval.eval(search = search, query_answers = query_answers, hits_k=range(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bert Sentence classifier improved the results of the extractor a little. We managed to increase the F1@k score, precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are proposed tags for Bert using retriever ranker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'uri': 'http://www.semanlink.net/tag/retrieval_augmented_lm',\n",
       "  'similarity': 0.5449117422103882},\n",
       " {'uri': 'http://www.semanlink.net/tag/neural_models_for_information_retrieval',\n",
       "  'similarity': 0.42808786034584045},\n",
       " {'uri': 'http://www.semanlink.net/tag/embeddings_in_ir',\n",
       "  'similarity': 0.42719531059265137},\n",
       " {'uri': 'http://www.semanlink.net/tag/dense_passage_retrieval',\n",
       "  'similarity': 0.4264187514781952},\n",
       " {'uri': 'http://www.semanlink.net/tag/information_retrieval',\n",
       "  'similarity': 0.40513238310813904},\n",
       " {'uri': 'http://www.semanlink.net/tag/entity_discovery_and_linking',\n",
       "  'similarity': 0.35288918018341064},\n",
       " {'uri': 'http://www.semanlink.net/tag/ranking_information_retrieval',\n",
       "  'similarity': 0.34628304839134216},\n",
       " {'uri': 'http://www.semanlink.net/tag/retrieval_based_nlp',\n",
       "  'similarity': 0.32937100529670715},\n",
       " {'uri': 'http://www.semanlink.net/tag/active_learning',\n",
       "  'similarity': 0.2809343934059143},\n",
       " {'uri': 'http://www.semanlink.net/tag/cognitive_search',\n",
       "  'similarity': 0.25938060879707336}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(\"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to use using Flash as a retriever. Flash Text will retrieve tags labels inside the title. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranker embeddings calculation.: 100%|█| 7/7 [00:03<00:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Flash retriever\n",
       " \t key: uri\n",
       " \t on: prefLabel, altLabel\n",
       " \t documents: 605\n",
       "Encoder ranker\n",
       "\t key: uri\n",
       "\t on: prefLabel_text, altLabel_text\n",
       "\t k: 10\n",
       "\t similarity: cosine\n",
       "\t Embeddings pre-computed: 433"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = retrieve.Flash(\n",
    "    key = \"uri\",\n",
    "    on = [\"prefLabel\", \"altLabel\"], \n",
    "    k = 30,\n",
    ")\n",
    "\n",
    "search = retriever + ranker\n",
    "search.add(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FlashText as a retriever provides fewer candidates than TfIdf but has higher precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Precision@1': '72.80%',\n",
       " 'Precision@2': '61.90%',\n",
       " 'Precision@3': '59.90%',\n",
       " 'Precision@4': '59.27%',\n",
       " 'Precision@5': '59.37%',\n",
       " 'Recall@1': '16.33%',\n",
       " 'Recall@2': '19.54%',\n",
       " 'Recall@3': '20.11%',\n",
       " 'Recall@4': '20.16%',\n",
       " 'Recall@5': '20.20%',\n",
       " 'F1@1': '26.67%',\n",
       " 'F1@2': '29.71%',\n",
       " 'F1@3': '30.11%',\n",
       " 'F1@4': '30.08%',\n",
       " 'F1@5': '30.15%',\n",
       " 'R-Precision': '20.20%',\n",
       " 'Precision': '59.37%'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval.eval(search = search, query_answers = query_answers, hits_k=range(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(\"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the best of both worlds by using pipeline union. It gets a bit complicated, but the union allows us to retrieve the best candidates from the first model then add the candidates from the second model without duplicates (no matter how many models are in the union). Our first retriever and ranker (Flash + Encoder) have low recall and high precision. The second retriever has a lower precision but higher recall. So we can mix things up and offer Flash and Ranker candidates first, then TfIdf and Ranker candidates seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranker embeddings calculation.: 100%|█| 7/7 [00:03<00:\n",
      "Ranker embeddings calculation.: 100%|█| 7/7 [00:03<00:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Union Pipeline\n",
       "-----\n",
       "Flash retriever\n",
       " \t key: uri\n",
       " \t on: prefLabel, altLabel\n",
       " \t documents: 605\n",
       "Encoder ranker\n",
       "\t key: uri\n",
       "\t on: prefLabel_text, altLabel_text\n",
       "\t k: 10\n",
       "\t similarity: cosine\n",
       "\t Embeddings pre-computed: 433\n",
       "TfIdf retriever\n",
       " \t key: uri\n",
       " \t on: prefLabel_text, altLabel_text\n",
       " \t documents: 433\n",
       "Encoder ranker\n",
       "\t key: uri\n",
       "\t on: prefLabel_text, altLabel_text\n",
       "\t k: 10\n",
       "\t similarity: cosine\n",
       "\t Embeddings pre-computed: 433\n",
       "-----"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = retrieve.Flash(\n",
    "    key = \"uri\",\n",
    "    on = [\"prefLabel\", \"altLabel\"], \n",
    "    k = 30,\n",
    ") + ranker\n",
    "\n",
    "recall = retrieve.TfIdf(\n",
    "    key = \"uri\",\n",
    "    on = [\"prefLabel_text\", \"altLabel_text\"], \n",
    "    documents = documents,\n",
    "    tfidf = TfidfVectorizer(lowercase=True, min_df=1, max_df=0.9, ngram_range=(3, 7), analyzer=\"char\"), \n",
    "    k = 30,\n",
    ") + ranker\n",
    "\n",
    "search = precision | recall\n",
    "\n",
    "search.add(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Precision@1': '69.11%',\n",
       " 'Precision@2': '49.84%',\n",
       " 'Precision@3': '39.07%',\n",
       " 'Precision@4': '31.13%',\n",
       " 'Precision@5': '25.92%',\n",
       " 'Recall@1': '18.74%',\n",
       " 'Recall@2': '25.89%',\n",
       " 'Recall@3': '30.10%',\n",
       " 'Recall@4': '31.58%',\n",
       " 'Recall@5': '32.57%',\n",
       " 'F1@1': '29.49%',\n",
       " 'F1@2': '34.08%',\n",
       " 'F1@3': '34.00%',\n",
       " 'F1@4': '31.35%',\n",
       " 'F1@5': '28.87%',\n",
       " 'R-Precision': '31.96%',\n",
       " 'Precision': '13.97%'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval.eval(search = search, query_answers = query_answers, hits_k=range(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did improves F1 and recall scores using union of pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are our tags for BERT's article with best of both worlds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'uri': 'http://www.semanlink.net/tag/retrieval_augmented_lm',\n",
       "  'similarity': 0.11754539040261458},\n",
       " {'uri': 'http://www.semanlink.net/tag/neural_models_for_information_retrieval',\n",
       "  'similarity': 0.10458505653003734},\n",
       " {'uri': 'http://www.semanlink.net/tag/embeddings_in_ir',\n",
       "  'similarity': 0.10449175080983726},\n",
       " {'uri': 'http://www.semanlink.net/tag/dense_passage_retrieval',\n",
       "  'similarity': 0.10441063828677113},\n",
       " {'uri': 'http://www.semanlink.net/tag/information_retrieval',\n",
       "  'similarity': 0.10221160275170203},\n",
       " {'uri': 'http://www.semanlink.net/tag/entity_discovery_and_linking',\n",
       "  'similarity': 0.09700882931829885},\n",
       " {'uri': 'http://www.semanlink.net/tag/ranking_information_retrieval',\n",
       "  'similarity': 0.0963700883333301},\n",
       " {'uri': 'http://www.semanlink.net/tag/retrieval_based_nlp',\n",
       "  'similarity': 0.09475397763274808},\n",
       " {'uri': 'http://www.semanlink.net/tag/active_learning',\n",
       "  'similarity': 0.09027379432403294},\n",
       " {'uri': 'http://www.semanlink.net/tag/cognitive_search',\n",
       "  'similarity': 0.08834887161062754}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(\"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b170744ab9cf7446ed3e27cb2734f2273f9ffda6b52a7d603d13471f7231bb1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
